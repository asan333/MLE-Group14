{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe2ZcUhStEzJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe2ZcUhStEzJ",
        "outputId": "164bf6c9-db6a-4fba-a983-65039ad199cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i7KxNPbZtM-Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7KxNPbZtM-Y",
        "outputId": "9d3d1b51-8466-4dcc-bb91-ef26a60e5e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 15549, done.\u001b[K\n",
            "remote: Counting objects: 100% (274/274), done.\u001b[K\n",
            "remote: Compressing objects: 100% (211/211), done.\u001b[K\n",
            "remote: Total 15549 (delta 92), reused 202 (delta 63), pack-reused 15275\u001b[K\n",
            "Receiving objects: 100% (15549/15549), 6.41 MiB | 12.37 MiB/s, done.\n",
            "Resolving deltas: 100% (11203/11203), done.\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf<2.4,>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-24.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.2)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.4,>=2.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.11.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=3c36bb15cf77bf2ce9cb4d704531c07e15984f7fc9822ff255fdc21e78461279\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=34e07e4181b7a03b930928be684434bfeeaf590231bd96bdde62c10512869811\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-24.4.1 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.8.2 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "import cv2\n",
        "from collections import OrderedDict\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data.catalog import DatasetCatalog\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, hooks, launch\n",
        "from detectron2.evaluation import CityscapesInstanceEvaluator\n",
        "from detectron2.evaluation import CityscapesSemSegEvaluator\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "from detectron2.evaluation import COCOPanopticEvaluator\n",
        "from detectron2.evaluation import DatasetEvaluators\n",
        "from detectron2.evaluation import LVISEvaluator\n",
        "from detectron2.evaluation import PascalVOCDetectionEvaluator\n",
        "from detectron2.evaluation import SemSegEvaluator\n",
        "from detectron2.evaluation import verify_results\n",
        "from detectron2.modeling import GeneralizedRCNNWithTTA\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "import random\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.engine import DefaultPredictor\n",
        "import cv2\n",
        "from demo.predictor import VisualizationDemo\n",
        "from detectron2.utils.visualizer import ColorMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64873636dac1a794",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64873636dac1a794",
        "outputId": "63ec0721-97dd-4bdb-8bf8-131c9c35ef36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/24 15:53:17 d2.data.datasets.coco]: Loading /content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/train/_annotations.coco.json takes 8.29 seconds.\n",
            "WARNING [04/24 15:53:17 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[04/24 15:53:17 d2.data.datasets.coco]: Loaded 498 images in COCO format from /content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/train/_annotations.coco.json\n"
          ]
        }
      ],
      "source": [
        "register_coco_instances(\"my_dataset_train\", {}, \"/content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/train/_annotations.coco.json\", \"/content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/train\")\n",
        "register_coco_instances(\"my_dataset_val\", {}, \"/content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/valid/_annotations.coco.json\", \"/content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/valid\")\n",
        "register_coco_instances(\"my_dataset_test\", {},\"/content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/test/_annotations.coco.json\", \"/content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/test\")\n",
        "my_dataset_train_metadata = MetadataCatalog.get(\"my_dataset_train\").set(\n",
        "    thing_classes=['nuclei-Jf4c', 'Nuclei',])\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92654a93b74d0910",
      "metadata": {
        "id": "92654a93b74d0910"
      },
      "outputs": [],
      "source": [
        "def build_evaluator(cfg, dataset_name, output_folder=None):\n",
        "    if output_folder is None:\n",
        "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "    evaluator_list = []\n",
        "    evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
        "    if evaluator_type in [\"sem_seg\", \"coco_panoptic_seg\"]:\n",
        "        evaluator_list.append(\n",
        "            SemSegEvaluator(\n",
        "                dataset_name,\n",
        "                distributed=True,\n",
        "                output_dir=output_folder,\n",
        "            )\n",
        "        )\n",
        "    if evaluator_type in [\"coco\", \"coco_panoptic_seg\"]:\n",
        "        evaluator_list.append(COCOEvaluator(dataset_name, output_dir=output_folder))\n",
        "    if evaluator_type == \"coco_panoptic_seg\":\n",
        "        evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n",
        "    if evaluator_type == \"cityscapes_instance\":\n",
        "        return CityscapesInstanceEvaluator(dataset_name)\n",
        "    if evaluator_type == \"cityscapes_sem_seg\":\n",
        "        return CityscapesSemSegEvaluator(dataset_name)\n",
        "    elif evaluator_type == \"pascal_voc\":\n",
        "        return PascalVOCDetectionEvaluator(dataset_name)\n",
        "    elif evaluator_type == \"lvis\":\n",
        "        return LVISEvaluator(dataset_name, output_dir=output_folder)\n",
        "    if len(evaluator_list) == 0:\n",
        "        raise NotImplementedError(\n",
        "            \"no Evaluator for the dataset {} with the type {}\".format(dataset_name, evaluator_type)\n",
        "        )\n",
        "    elif len(evaluator_list) == 1:\n",
        "        return evaluator_list[0]\n",
        "    return DatasetEvaluators(evaluator_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d561a67d75e058",
      "metadata": {
        "id": "d2d561a67d75e058"
      },
      "outputs": [],
      "source": [
        "class Trainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        return build_evaluator(cfg, dataset_name, output_folder)\n",
        "\n",
        "    @classmethod\n",
        "    def test_with_TTA(cls, cfg, model):\n",
        "        logger = logging.getLogger(\"detectron2.trainer\")\n",
        "        logger.info(\"Running inference with test-time augmentation ...\")\n",
        "        model = GeneralizedRCNNWithTTA(cfg, model)\n",
        "        evaluators = [\n",
        "            cls.build_evaluator(\n",
        "                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n",
        "            )\n",
        "            for name in cfg.DATASETS.TEST\n",
        "        ]\n",
        "        res = cls.test(cfg, model, evaluators)\n",
        "        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa9d2df895c9c41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaa9d2df895c9c41",
        "outputId": "0bcd9837-bc07-44aa-e6ee-e2605b9fe0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/24 16:38:56 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "WARNING [04/24 16:38:56 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[04/24 16:38:56 d2.data.datasets.coco]: Loaded 498 images in COCO format from /content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/train/_annotations.coco.json\n",
            "[04/24 16:38:56 d2.data.build]: Removed 0 images with no usable annotations. 498 images left.\n",
            "[04/24 16:38:56 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [RandomCrop(crop_type='relative_range', crop_size=[0.9, 0.9]), ResizeShortestEdge(short_edge_length=(512, 512), max_size=768, sample_style='choice'), RandomFlip()]\n",
            "[04/24 16:38:56 d2.data.build]: Using training sampler TrainingSampler\n",
            "[04/24 16:38:56 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 16:38:56 d2.data.common]: Serializing 498 elements to byte tensors and concatenating them all ...\n",
            "[04/24 16:38:56 d2.data.common]: Serialized dataset takes 3.97 MiB\n",
            "[04/24 16:38:56 d2.data.build]: Making batched data loader with batch_size=16\n",
            "WARNING [04/24 16:38:56 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "[04/24 16:38:56 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (2, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n"
          ]
        }
      ],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 1\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.INPUT.CROP.ENABLED = True\n",
        "cfg.INPUT.MAX_SIZE_TRAIN = 768\n",
        "cfg.INPUT.MAX_SIZE_TEST = 768\n",
        "cfg.INPUT.MIN_SIZE_TRAIN = 512\n",
        "cfg.INPUT.MIN_SIZE_TEST = 512\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n",
        "cfg.MODEL.RETINANET.NUM_CLASSES = 3\n",
        "ITERS_IN_ONE_EPOCH = int(9000 / cfg.SOLVER.IMS_PER_BATCH)\n",
        "\n",
        "# cfg.SOLVER.MAX_ITER = (ITERS_IN_ONE_EPOCH * 12) - 1\n",
        "cfg.SOLVER.MAX_ITER = ITERS_IN_ONE_EPOCH- 1\n",
        "cfg.SOLVER.BASE_LR = 0.0025\n",
        "cfg.SOLVER.MOMENTUM = 0.9\n",
        "cfg.SOLVER.WEIGHT_DECAY = 0.0001\n",
        "cfg.SOLVER.WEIGHT_DECAY_NORM = 0.0\n",
        "cfg.SOLVER.GAMMA = 0.1\n",
        "cfg.SOLVER.STEPS = (7000,)\n",
        "cfg.SOLVER.WARMUP_FACTOR = 1.0 / 1000\n",
        "cfg.SOLVER.WARMUP_ITERS = 1000\n",
        "\n",
        "cfg.SOLVER.WARMUP_METHOD = \"linear\"\n",
        "\n",
        "cfg.TEST.EVAL_PERIOD = 500\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = Trainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "if cfg.TEST.AUG.ENABLED:\n",
        "    trainer.register_hooks(\n",
        "        [hooks.EvalHook(0, lambda: trainer.test_with_TTA(cfg, trainer.model))]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-JXN_Cb-WGX",
        "outputId": "522b67e2-9846-471c-95e1-35482a8cf8eb"
      },
      "id": "x-JXN_Cb-WGX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/24 16:39:01 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/24 16:39:50 d2.utils.events]:  eta: 0:22:20  iter: 19  total_loss: 4.895  loss_cls: 1.04  loss_box_reg: 0.744  loss_mask: 0.686  loss_rpn_cls: 2.077  loss_rpn_loc: 0.3817    time: 2.4427  last_time: 2.3539  data_time: 0.8736  last_data_time: 0.6086   lr: 8.7086e-05  max_mem: 7470M\n",
            "[04/24 16:40:39 d2.utils.events]:  eta: 0:21:17  iter: 39  total_loss: 2.465  loss_cls: 0.5945  loss_box_reg: 0.7515  loss_mask: 0.54  loss_rpn_cls: 0.284  loss_rpn_loc: 0.3395    time: 2.4443  last_time: 3.5608  data_time: 0.8765  last_data_time: 1.9482   lr: 0.00017612  max_mem: 7470M\n",
            "[04/24 16:41:25 d2.utils.events]:  eta: 0:19:41  iter: 59  total_loss: 1.976  loss_cls: 0.4039  loss_box_reg: 0.7179  loss_mask: 0.4049  loss_rpn_cls: 0.1677  loss_rpn_loc: 0.2678    time: 2.3888  last_time: 2.6875  data_time: 0.7100  last_data_time: 1.1782   lr: 0.00026516  max_mem: 7470M\n",
            "[04/24 16:42:13 d2.utils.events]:  eta: 0:18:47  iter: 79  total_loss: 1.687  loss_cls: 0.3057  loss_box_reg: 0.6237  loss_mask: 0.3387  loss_rpn_cls: 0.1417  loss_rpn_loc: 0.2646    time: 2.3894  last_time: 2.1964  data_time: 0.8110  last_data_time: 0.4999   lr: 0.0003542  max_mem: 7470M\n",
            "[04/24 16:43:01 d2.utils.events]:  eta: 0:17:59  iter: 99  total_loss: 1.457  loss_cls: 0.2451  loss_box_reg: 0.5082  loss_mask: 0.2974  loss_rpn_cls: 0.135  loss_rpn_loc: 0.2461    time: 2.3934  last_time: 2.1790  data_time: 0.8415  last_data_time: 0.7184   lr: 0.00044324  max_mem: 7470M\n",
            "[04/24 16:43:49 d2.utils.events]:  eta: 0:17:20  iter: 119  total_loss: 1.305  loss_cls: 0.2265  loss_box_reg: 0.4338  loss_mask: 0.2628  loss_rpn_cls: 0.1226  loss_rpn_loc: 0.2499    time: 2.3956  last_time: 3.2309  data_time: 0.8404  last_data_time: 1.6419   lr: 0.00053227  max_mem: 7471M\n",
            "[04/24 16:44:37 d2.utils.events]:  eta: 0:16:36  iter: 139  total_loss: 1.205  loss_cls: 0.2047  loss_box_reg: 0.4024  loss_mask: 0.2471  loss_rpn_cls: 0.1147  loss_rpn_loc: 0.2252    time: 2.3938  last_time: 1.9446  data_time: 0.7759  last_data_time: 0.2021   lr: 0.00062131  max_mem: 7471M\n",
            "[04/24 16:45:26 d2.utils.events]:  eta: 0:15:53  iter: 159  total_loss: 1.264  loss_cls: 0.216  loss_box_reg: 0.4095  loss_mask: 0.2544  loss_rpn_cls: 0.116  loss_rpn_loc: 0.2539    time: 2.4018  last_time: 2.8903  data_time: 0.8769  last_data_time: 1.2151   lr: 0.00071035  max_mem: 7471M\n",
            "[04/24 16:46:14 d2.utils.events]:  eta: 0:15:02  iter: 179  total_loss: 1.134  loss_cls: 0.1909  loss_box_reg: 0.4063  loss_mask: 0.25  loss_rpn_cls: 0.09932  loss_rpn_loc: 0.2228    time: 2.4037  last_time: 1.6465  data_time: 0.7937  last_data_time: 0.1190   lr: 0.00079939  max_mem: 7471M\n",
            "[04/24 16:47:03 d2.utils.events]:  eta: 0:14:18  iter: 199  total_loss: 1.101  loss_cls: 0.1755  loss_box_reg: 0.3791  loss_mask: 0.2355  loss_rpn_cls: 0.09056  loss_rpn_loc: 0.1968    time: 2.4065  last_time: 1.7400  data_time: 0.8471  last_data_time: 0.0661   lr: 0.00088842  max_mem: 7471M\n",
            "[04/24 16:47:53 d2.utils.events]:  eta: 0:13:39  iter: 219  total_loss: 1.21  loss_cls: 0.1906  loss_box_reg: 0.3864  loss_mask: 0.249  loss_rpn_cls: 0.1128  loss_rpn_loc: 0.2447    time: 2.4175  last_time: 3.1734  data_time: 0.9260  last_data_time: 1.3742   lr: 0.00097746  max_mem: 7471M\n",
            "[04/24 16:48:40 d2.utils.events]:  eta: 0:12:51  iter: 239  total_loss: 1.118  loss_cls: 0.181  loss_box_reg: 0.3733  loss_mask: 0.2399  loss_rpn_cls: 0.0971  loss_rpn_loc: 0.215    time: 2.4107  last_time: 2.0364  data_time: 0.7698  last_data_time: 0.5184   lr: 0.0010665  max_mem: 7471M\n",
            "[04/24 16:49:28 d2.utils.events]:  eta: 0:12:03  iter: 259  total_loss: 1.073  loss_cls: 0.1697  loss_box_reg: 0.3777  loss_mask: 0.2369  loss_rpn_cls: 0.08349  loss_rpn_loc: 0.2138    time: 2.4110  last_time: 2.2298  data_time: 0.8326  last_data_time: 0.6350   lr: 0.0011555  max_mem: 7471M\n",
            "[04/24 16:50:18 d2.utils.events]:  eta: 0:11:13  iter: 279  total_loss: 1.079  loss_cls: 0.167  loss_box_reg: 0.3792  loss_mask: 0.2382  loss_rpn_cls: 0.09826  loss_rpn_loc: 0.2085    time: 2.4139  last_time: 2.6848  data_time: 0.8626  last_data_time: 1.1593   lr: 0.0012446  max_mem: 7471M\n",
            "[04/24 16:51:05 d2.utils.events]:  eta: 0:10:24  iter: 299  total_loss: 1.1  loss_cls: 0.1713  loss_box_reg: 0.3681  loss_mask: 0.2401  loss_rpn_cls: 0.0942  loss_rpn_loc: 0.2127    time: 2.4120  last_time: 2.7084  data_time: 0.8074  last_data_time: 1.1651   lr: 0.0013336  max_mem: 7471M\n",
            "[04/24 16:51:52 d2.utils.events]:  eta: 0:09:38  iter: 319  total_loss: 1.064  loss_cls: 0.1684  loss_box_reg: 0.3705  loss_mask: 0.2298  loss_rpn_cls: 0.07839  loss_rpn_loc: 0.2023    time: 2.4082  last_time: 1.4832  data_time: 0.7705  last_data_time: 0.0125   lr: 0.0014226  max_mem: 7471M\n",
            "[04/24 16:52:43 d2.utils.events]:  eta: 0:08:52  iter: 339  total_loss: 1.083  loss_cls: 0.1633  loss_box_reg: 0.3599  loss_mask: 0.2279  loss_rpn_cls: 0.08448  loss_rpn_loc: 0.222    time: 2.4151  last_time: 2.5644  data_time: 0.9365  last_data_time: 1.1144   lr: 0.0015117  max_mem: 7471M\n",
            "[04/24 16:53:31 d2.utils.events]:  eta: 0:08:04  iter: 359  total_loss: 1.047  loss_cls: 0.1625  loss_box_reg: 0.3671  loss_mask: 0.2266  loss_rpn_cls: 0.0832  loss_rpn_loc: 0.2148    time: 2.4134  last_time: 2.8998  data_time: 0.7926  last_data_time: 1.3770   lr: 0.0016007  max_mem: 7471M\n",
            "[04/24 16:54:19 d2.utils.events]:  eta: 0:07:15  iter: 379  total_loss: 1.062  loss_cls: 0.1718  loss_box_reg: 0.3577  loss_mask: 0.2287  loss_rpn_cls: 0.07987  loss_rpn_loc: 0.2143    time: 2.4136  last_time: 1.7980  data_time: 0.8100  last_data_time: 0.1076   lr: 0.0016898  max_mem: 7471M\n",
            "[04/24 16:55:07 d2.utils.events]:  eta: 0:06:28  iter: 399  total_loss: 1.054  loss_cls: 0.1734  loss_box_reg: 0.3601  loss_mask: 0.232  loss_rpn_cls: 0.08201  loss_rpn_loc: 0.1986    time: 2.4140  last_time: 2.7011  data_time: 0.8219  last_data_time: 1.1233   lr: 0.0017788  max_mem: 7471M\n",
            "[04/24 16:55:57 d2.utils.events]:  eta: 0:05:40  iter: 419  total_loss: 1.042  loss_cls: 0.1683  loss_box_reg: 0.3591  loss_mask: 0.2275  loss_rpn_cls: 0.07647  loss_rpn_loc: 0.2103    time: 2.4161  last_time: 3.0208  data_time: 0.8502  last_data_time: 1.4880   lr: 0.0018678  max_mem: 7471M\n",
            "[04/24 16:56:45 d2.utils.events]:  eta: 0:04:52  iter: 439  total_loss: 1.017  loss_cls: 0.1481  loss_box_reg: 0.3481  loss_mask: 0.2261  loss_rpn_cls: 0.0769  loss_rpn_loc: 0.2031    time: 2.4157  last_time: 2.1198  data_time: 0.8046  last_data_time: 0.5169   lr: 0.0019569  max_mem: 7471M\n",
            "[04/24 16:57:33 d2.utils.events]:  eta: 0:04:03  iter: 459  total_loss: 1.061  loss_cls: 0.1537  loss_box_reg: 0.3663  loss_mask: 0.2272  loss_rpn_cls: 0.08266  loss_rpn_loc: 0.2052    time: 2.4146  last_time: 2.3229  data_time: 0.8207  last_data_time: 0.8014   lr: 0.0020459  max_mem: 7471M\n",
            "[04/24 16:58:22 d2.utils.events]:  eta: 0:03:15  iter: 479  total_loss: 1.066  loss_cls: 0.1785  loss_box_reg: 0.3646  loss_mask: 0.2305  loss_rpn_cls: 0.07516  loss_rpn_loc: 0.2154    time: 2.4179  last_time: 2.6101  data_time: 0.9052  last_data_time: 1.1540   lr: 0.0021349  max_mem: 7472M\n",
            "[04/24 16:59:21 d2.data.datasets.coco]: Loading /content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/valid/_annotations.coco.json takes 11.03 seconds.\n",
            "WARNING [04/24 16:59:21 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[04/24 16:59:21 d2.data.datasets.coco]: Loaded 166 images in COCO format from /content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/valid/_annotations.coco.json\n",
            "[04/24 16:59:21 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|  category   | #instances   |  category  | #instances   |\n",
            "|:-----------:|:-------------|:----------:|:-------------|\n",
            "| nuclei-Jf4c | 0            |   Nuclei   | 7148         |\n",
            "|             |              |            |              |\n",
            "|    total    | 7148         |            |              |\n",
            "[04/24 16:59:21 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=768, sample_style='choice')]\n",
            "[04/24 16:59:21 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 16:59:21 d2.data.common]: Serializing 166 elements to byte tensors and concatenating them all ...\n",
            "[04/24 16:59:21 d2.data.common]: Serialized dataset takes 1.30 MiB\n",
            "[04/24 16:59:21 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[04/24 16:59:22 d2.evaluation.evaluator]: Start inference on 166 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/24 16:59:26 d2.evaluation.evaluator]: Inference done 11/166. Dataloading: 0.2302 s/iter. Inference: 0.0842 s/iter. Eval: 0.0149 s/iter. Total: 0.3293 s/iter. ETA=0:00:51\n",
            "[04/24 16:59:32 d2.evaluation.evaluator]: Inference done 24/166. Dataloading: 0.2854 s/iter. Inference: 0.0838 s/iter. Eval: 0.0203 s/iter. Total: 0.3897 s/iter. ETA=0:00:55\n",
            "[04/24 16:59:37 d2.evaluation.evaluator]: Inference done 36/166. Dataloading: 0.3066 s/iter. Inference: 0.0846 s/iter. Eval: 0.0192 s/iter. Total: 0.4105 s/iter. ETA=0:00:53\n",
            "[04/24 16:59:46 d2.evaluation.evaluator]: Inference done 42/166. Dataloading: 0.4965 s/iter. Inference: 0.0825 s/iter. Eval: 0.0172 s/iter. Total: 0.5965 s/iter. ETA=0:01:13\n",
            "[04/24 16:59:52 d2.evaluation.evaluator]: Inference done 55/166. Dataloading: 0.4452 s/iter. Inference: 0.0826 s/iter. Eval: 0.0199 s/iter. Total: 0.5478 s/iter. ETA=0:01:00\n",
            "[04/24 16:59:57 d2.evaluation.evaluator]: Inference done 60/166. Dataloading: 0.4943 s/iter. Inference: 0.0823 s/iter. Eval: 0.0184 s/iter. Total: 0.5952 s/iter. ETA=0:01:03\n",
            "[04/24 17:00:03 d2.evaluation.evaluator]: Inference done 73/166. Dataloading: 0.4546 s/iter. Inference: 0.0820 s/iter. Eval: 0.0246 s/iter. Total: 0.5614 s/iter. ETA=0:00:52\n",
            "[04/24 17:00:15 d2.evaluation.evaluator]: Inference done 84/166. Dataloading: 0.5265 s/iter. Inference: 0.0814 s/iter. Eval: 0.0262 s/iter. Total: 0.6344 s/iter. ETA=0:00:52\n",
            "[04/24 17:00:20 d2.evaluation.evaluator]: Inference done 96/166. Dataloading: 0.5000 s/iter. Inference: 0.0814 s/iter. Eval: 0.0252 s/iter. Total: 0.6069 s/iter. ETA=0:00:42\n",
            "[04/24 17:00:25 d2.evaluation.evaluator]: Inference done 109/166. Dataloading: 0.4748 s/iter. Inference: 0.0809 s/iter. Eval: 0.0243 s/iter. Total: 0.5801 s/iter. ETA=0:00:33\n",
            "[04/24 17:00:30 d2.evaluation.evaluator]: Inference done 122/166. Dataloading: 0.4535 s/iter. Inference: 0.0807 s/iter. Eval: 0.0255 s/iter. Total: 0.5600 s/iter. ETA=0:00:24\n",
            "[04/24 17:00:40 d2.evaluation.evaluator]: Inference done 143/166. Dataloading: 0.4382 s/iter. Inference: 0.0813 s/iter. Eval: 0.0291 s/iter. Total: 0.5488 s/iter. ETA=0:00:12\n",
            "[04/24 17:00:45 d2.evaluation.evaluator]: Inference done 155/166. Dataloading: 0.4287 s/iter. Inference: 0.0816 s/iter. Eval: 0.0285 s/iter. Total: 0.5391 s/iter. ETA=0:00:05\n",
            "[04/24 17:00:50 d2.evaluation.evaluator]: Total inference time: 0:01:25.121512 (0.528705 s / iter per device, on 1 devices)\n",
            "[04/24 17:00:50 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:13 (0.081363 s / iter per device, on 1 devices)\n",
            "[04/24 17:00:50 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[04/24 17:00:50 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[04/24 17:00:50 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=9.57s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.738\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.551\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.437\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.530\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.813\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            "[04/24 17:01:00 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
            "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
            "| 48.462 | 73.751 | 55.144 | 43.714 | 77.452 |  nan  |\n",
            "[04/24 17:01:00 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[04/24 17:01:00 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category    | AP   | category   | AP     |\n",
            "|:------------|:-----|:-----------|:-------|\n",
            "| nuclei-Jf4c | nan  | Nuclei     | 48.462 |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.05s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=10.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.492\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.738\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.550\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.444\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.789\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.537\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.492\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.816\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            "[04/24 17:01:10 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
            "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
            "| 49.211 | 73.776 | 55.046 | 44.371 | 78.880 |  nan  |\n",
            "[04/24 17:01:10 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[04/24 17:01:10 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category    | AP   | category   | AP     |\n",
            "|:------------|:-----|:-----------|:-------|\n",
            "| nuclei-Jf4c | nan  | Nuclei     | 49.211 |\n",
            "[04/24 17:01:10 d2.engine.defaults]: Evaluation results for my_dataset_val in csv format:\n",
            "[04/24 17:01:10 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[04/24 17:01:10 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 17:01:10 d2.evaluation.testing]: copypaste: 48.4615,73.7510,55.1441,43.7140,77.4524,nan\n",
            "[04/24 17:01:10 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[04/24 17:01:10 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 17:01:10 d2.evaluation.testing]: copypaste: 49.2110,73.7763,55.0462,44.3713,78.8802,nan\n",
            "[04/24 17:01:11 d2.utils.events]:  eta: 0:02:27  iter: 499  total_loss: 1.002  loss_cls: 0.1484  loss_box_reg: 0.3411  loss_mask: 0.2216  loss_rpn_cls: 0.06945  loss_rpn_loc: 0.1998    time: 2.4171  last_time: 2.0309  data_time: 0.7960  last_data_time: 0.3443   lr: 0.002224  max_mem: 7472M\n",
            "[04/24 17:01:59 d2.utils.events]:  eta: 0:01:38  iter: 519  total_loss: 1.028  loss_cls: 0.1545  loss_box_reg: 0.3668  loss_mask: 0.2184  loss_rpn_cls: 0.0731  loss_rpn_loc: 0.2009    time: 2.4180  last_time: 2.5585  data_time: 0.8379  last_data_time: 0.9709   lr: 0.002313  max_mem: 7472M\n",
            "[04/24 17:02:47 d2.utils.events]:  eta: 0:00:50  iter: 539  total_loss: 0.9951  loss_cls: 0.1446  loss_box_reg: 0.3493  loss_mask: 0.2201  loss_rpn_cls: 0.07078  loss_rpn_loc: 0.2073    time: 2.4176  last_time: 2.7074  data_time: 0.8350  last_data_time: 1.1947   lr: 0.0024021  max_mem: 7472M\n",
            "[04/24 17:03:34 d2.utils.events]:  eta: 0:00:02  iter: 559  total_loss: 0.999  loss_cls: 0.1535  loss_box_reg: 0.344  loss_mask: 0.2191  loss_rpn_cls: 0.07006  loss_rpn_loc: 0.2058    time: 2.4145  last_time: 2.0113  data_time: 0.7395  last_data_time: 0.3943   lr: 0.0024911  max_mem: 7472M\n",
            "[04/24 17:03:37 d2.utils.events]:  eta: 0:00:00  iter: 560  total_loss: 0.999  loss_cls: 0.1494  loss_box_reg: 0.344  loss_mask: 0.2191  loss_rpn_cls: 0.07006  loss_rpn_loc: 0.2058    time: 2.4141  last_time: 2.1562  data_time: 0.7765  last_data_time: 0.7577   lr: 0.0024955  max_mem: 7472M\n",
            "[04/24 17:03:38 d2.engine.hooks]: Overall training speed: 559 iterations in 0:22:29 (2.4141 s / it)\n",
            "[04/24 17:03:38 d2.engine.hooks]: Total training time: 0:24:31 (0:02:02 on hooks)\n",
            "WARNING [04/24 17:03:38 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[04/24 17:03:38 d2.data.datasets.coco]: Loaded 166 images in COCO format from /content/drive/MyDrive/MLE_Group14/dsb2018_t.v1i.coco-segmentation/valid/_annotations.coco.json\n",
            "[04/24 17:03:38 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=768, sample_style='choice')]\n",
            "[04/24 17:03:38 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 17:03:38 d2.data.common]: Serializing 166 elements to byte tensors and concatenating them all ...\n",
            "[04/24 17:03:38 d2.data.common]: Serialized dataset takes 1.30 MiB\n",
            "[04/24 17:03:38 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[04/24 17:03:38 d2.evaluation.evaluator]: Start inference on 166 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/24 17:03:39 d2.evaluation.evaluator]: Inference done 11/166. Dataloading: 0.0011 s/iter. Inference: 0.0567 s/iter. Eval: 0.0112 s/iter. Total: 0.0690 s/iter. ETA=0:00:10\n",
            "[04/24 17:03:44 d2.evaluation.evaluator]: Inference done 68/166. Dataloading: 0.0015 s/iter. Inference: 0.0596 s/iter. Eval: 0.0252 s/iter. Total: 0.0864 s/iter. ETA=0:00:08\n",
            "[04/24 17:03:49 d2.evaluation.evaluator]: Inference done 116/166. Dataloading: 0.0026 s/iter. Inference: 0.0624 s/iter. Eval: 0.0291 s/iter. Total: 0.0942 s/iter. ETA=0:00:04\n",
            "[04/24 17:03:54 d2.evaluation.evaluator]: Total inference time: 0:00:15.246301 (0.094698 s / iter per device, on 1 devices)\n",
            "[04/24 17:03:54 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:09 (0.061813 s / iter per device, on 1 devices)\n",
            "[04/24 17:03:54 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[04/24 17:03:54 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[04/24 17:03:54 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=10.76s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.736\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.534\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.418\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.749\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.017\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.513\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.790\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            "[04/24 17:04:05 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
            "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
            "| 46.791 | 73.586 | 53.399 | 41.828 | 74.867 |  nan  |\n",
            "[04/24 17:04:05 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[04/24 17:04:05 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category    | AP   | category   | AP     |\n",
            "|:------------|:-----|:-----------|:-------|\n",
            "| nuclei-Jf4c | nan  | Nuclei     | 46.791 |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.05s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=10.73s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.737\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.538\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.430\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.778\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.804\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            "[04/24 17:04:16 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
            "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
            "| 48.116 | 73.746 | 53.842 | 42.961 | 77.803 |  nan  |\n",
            "[04/24 17:04:16 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[04/24 17:04:16 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category    | AP   | category   | AP     |\n",
            "|:------------|:-----|:-----------|:-------|\n",
            "| nuclei-Jf4c | nan  | Nuclei     | 48.116 |\n",
            "[04/24 17:04:16 d2.engine.defaults]: Evaluation results for my_dataset_val in csv format:\n",
            "[04/24 17:04:16 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[04/24 17:04:16 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 17:04:16 d2.evaluation.testing]: copypaste: 46.7908,73.5857,53.3991,41.8278,74.8666,nan\n",
            "[04/24 17:04:16 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[04/24 17:04:16 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 17:04:16 d2.evaluation.testing]: copypaste: 48.1162,73.7463,53.8416,42.9608,77.8032,nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30614058b95b4d9",
      "metadata": {
        "id": "a30614058b95b4d9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bce86bce34a998c",
      "metadata": {
        "id": "2bce86bce34a998c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "889074fd0b6d2df",
      "metadata": {
        "id": "889074fd0b6d2df"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}